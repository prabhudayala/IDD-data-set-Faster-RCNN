{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UauKNtFRqydu"
   },
   "source": [
    "### Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qSbY3Amlqpkh",
    "outputId": "785ace30-0652-460a-f5b7-2f6a877ad6ee"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "import random\n",
    "import pprint\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import pickle\n",
    "import math\n",
    "import cv2\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, TimeDistributed\n",
    "# from tensorflow.keras.engine.topology import get_source_inputs\n",
    "# from tensorflow.keras.utils import layer_utils\n",
    "# from tensorflow.keras.utils.data_utils import get_file\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "from tensorflow.keras import initializers, regularizers\n",
    "\n",
    "import faster_rcnn\n",
    "from faster_rcnn import RoiPoolingConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WrH5i5mmrDWY"
   },
   "source": [
    "#### Config setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DvJm0FFRsyVu"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # Print the process or not\n",
    "        self.verbose = True\n",
    "\n",
    "        # Name of base network\n",
    "        self.network = 'vgg'\n",
    "\n",
    "        # Setting for data augmentation\n",
    "        self.use_horizontal_flips = False\n",
    "        self.use_vertical_flips = False\n",
    "        self.rot_90 = False\n",
    "\n",
    "        # Anchor box scales\n",
    "    # Note that if im_size is smaller, anchor_box_scales should be scaled\n",
    "    # Original anchor_box_scales in the paper is [128, 256, 512]\n",
    "        self.anchor_box_scales = [64, 128, 256] \n",
    "\n",
    "        # Anchor box ratios\n",
    "        self.anchor_box_ratios = [[1, 1], [1./math.sqrt(2), 2./math.sqrt(2)], [2./math.sqrt(2), 1./math.sqrt(2)]]\n",
    "\n",
    "        # Size to resize the smallest side of the image\n",
    "        # Original setting in paper is 600. Set to 300 in here to save training time\n",
    "        self.im_size = 300\n",
    "\n",
    "        # image channel-wise mean to subtract\n",
    "        self.img_channel_mean = [103.939, 116.779, 123.68]\n",
    "        self.img_scaling_factor = 1.0\n",
    "\n",
    "        # number of ROIs at once\n",
    "        self.num_rois = 4\n",
    "\n",
    "        # stride at the RPN (this depends on the network configuration)\n",
    "        self.rpn_stride = 16\n",
    "\n",
    "        self.balanced_classes = False\n",
    "\n",
    "        # scaling the stdev\n",
    "        self.std_scaling = 4.0\n",
    "        self.classifier_regr_std = [8.0, 8.0, 4.0, 4.0]\n",
    "\n",
    "        # overlaps for RPN\n",
    "        self.rpn_min_overlap = 0.3\n",
    "        self.rpn_max_overlap = 0.7\n",
    "\n",
    "        # overlaps for classifier ROIs\n",
    "        self.classifier_min_overlap = 0.1\n",
    "        self.classifier_max_overlap = 0.5\n",
    "\n",
    "        # placeholder for the class mapping, automatically generated by the parser\n",
    "        self.class_mapping = None\n",
    "\n",
    "        self.model_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o0bIjlycyR9_"
   },
   "source": [
    "### Parser the data from annotation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vc89E9uAydTX"
   },
   "outputs": [],
   "source": [
    "def get_data(input_path):\n",
    "    \"\"\"Parse the data from annotation file\n",
    "\n",
    "    Args:\n",
    "        input_path: annotation file path\n",
    "\n",
    "    Returns:\n",
    "        all_data: list(filepath, width, height, list(bboxes))\n",
    "        classes_count: dict{key:class_name, value:count_num} \n",
    "            e.g. {'Car': 2383, 'Truck': 1108, 'Person': 3745}\n",
    "        class_mapping: dict{key:class_name, value: idx}\n",
    "            e.g. {'Car': 0, 'Truck': 1, 'Person': 2}\n",
    "    \"\"\"\n",
    "    found_bg = False\n",
    "    all_imgs = {}\n",
    "\n",
    "    classes_count = {}\n",
    "\n",
    "    class_mapping = {}\n",
    "\n",
    "    i = 1\n",
    "\n",
    "    with open(input_path,'r') as f:\n",
    "\n",
    "        print('Parsing annotation files')\n",
    "\n",
    "        for line in f:\n",
    "\n",
    "            # Print process\n",
    "            sys.stdout.write('\\r'+'idx=' + str(i))\n",
    "            i += 1\n",
    "\n",
    "            line_split = line.strip().split(',')\n",
    "\n",
    "            (filename,x1,y1,x2,y2,class_name) = line_split\n",
    "            #print(filename)          \n",
    "\n",
    "            if class_name not in classes_count:\n",
    "                classes_count[class_name] = 1\n",
    "            else:\n",
    "                classes_count[class_name] += 1\n",
    "\n",
    "            if class_name not in class_mapping:\n",
    "                if class_name == 'bg' and found_bg == False:\n",
    "                    print('Found class name with special name bg. Will be treated as a background region (this is usually for hard negative mining).')\n",
    "                    found_bg = True\n",
    "                class_mapping[class_name] = len(class_mapping)\n",
    "\n",
    "            if filename not in all_imgs:\n",
    "                all_imgs[filename] = {}\n",
    "                img = cv2.imread(filename)\n",
    "                (rows,cols) = img.shape[:2]\n",
    "                all_imgs[filename]['filepath'] = filename\n",
    "                all_imgs[filename]['width'] = cols\n",
    "                all_imgs[filename]['height'] = rows\n",
    "                all_imgs[filename]['bboxes'] = []\n",
    "\n",
    "                all_imgs[filename]['bboxes'].append({'class': class_name, 'x1': int(x1), 'x2': int(x2), 'y1': int(y1), 'y2': int(y2)})\n",
    "\n",
    "\n",
    "        all_data = []\n",
    "        for key in all_imgs:\n",
    "            all_data.append(all_imgs[key])\n",
    "\n",
    "        # make sure the bg class is last in the list\n",
    "        if found_bg:\n",
    "            if class_mapping['bg'] != len(class_mapping) - 1:\n",
    "                key_to_switch = [key for key in class_mapping.keys() if class_mapping[key] == len(class_mapping)-1][0]\n",
    "                val_to_switch = class_mapping['bg']\n",
    "                class_mapping['bg'] = len(class_mapping) - 1\n",
    "                class_mapping[key_to_switch] = val_to_switch\n",
    "\n",
    "        return all_data, classes_count, class_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3qGAalfJB8zz"
   },
   "source": [
    "#### Get new image size and augment the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HKhSFbmB2RTo"
   },
   "outputs": [],
   "source": [
    "def get_new_img_size(width, height, img_min_side=300):\n",
    "    if width <= height:\n",
    "        f = float(img_min_side) / width\n",
    "        resized_height = int(f * height)\n",
    "        resized_width = img_min_side\n",
    "    else:\n",
    "        f = float(img_min_side) / height\n",
    "        resized_width = int(f * width)\n",
    "        resized_height = img_min_side\n",
    "\n",
    "    return resized_width, resized_height\n",
    "\n",
    "def augment(img_data, config, augment=True):\n",
    "    assert 'filepath' in img_data\n",
    "    assert 'bboxes' in img_data\n",
    "    assert 'width' in img_data\n",
    "    assert 'height' in img_data\n",
    "\n",
    "    img_data_aug = copy.deepcopy(img_data)\n",
    "\n",
    "    img = cv2.imread(img_data_aug['filepath'])\n",
    "\n",
    "    if augment:\n",
    "        rows, cols = img.shape[:2]\n",
    "\n",
    "        if config.use_horizontal_flips and np.random.randint(0, 2) == 0:\n",
    "            img = cv2.flip(img, 1)\n",
    "            for bbox in img_data_aug['bboxes']:\n",
    "                x1 = bbox['x1']\n",
    "                x2 = bbox['x2']\n",
    "                bbox['x2'] = cols - x1\n",
    "                bbox['x1'] = cols - x2\n",
    "\n",
    "        if config.use_vertical_flips and np.random.randint(0, 2) == 0:\n",
    "            img = cv2.flip(img, 0)\n",
    "            for bbox in img_data_aug['bboxes']:\n",
    "                y1 = bbox['y1']\n",
    "                y2 = bbox['y2']\n",
    "                bbox['y2'] = rows - y1\n",
    "                bbox['y1'] = rows - y2\n",
    "\n",
    "        if config.rot_90:\n",
    "            angle = np.random.choice([0,90,180,270],1)[0]\n",
    "            if angle == 270:\n",
    "                img = np.transpose(img, (1,0,2))\n",
    "                img = cv2.flip(img, 0)\n",
    "            elif angle == 180:\n",
    "                img = cv2.flip(img, -1)\n",
    "            elif angle == 90:\n",
    "                img = np.transpose(img, (1,0,2))\n",
    "                img = cv2.flip(img, 1)\n",
    "            elif angle == 0:\n",
    "                pass\n",
    "\n",
    "            for bbox in img_data_aug['bboxes']:\n",
    "                x1 = bbox['x1']\n",
    "                x2 = bbox['x2']\n",
    "                y1 = bbox['y1']\n",
    "                y2 = bbox['y2']\n",
    "                if angle == 270:\n",
    "                    bbox['x1'] = y1\n",
    "                    bbox['x2'] = y2\n",
    "                    bbox['y1'] = cols - x2\n",
    "                    bbox['y2'] = cols - x1\n",
    "                elif angle == 180:\n",
    "                    bbox['x2'] = cols - x1\n",
    "                    bbox['x1'] = cols - x2\n",
    "                    bbox['y2'] = rows - y1\n",
    "                    bbox['y1'] = rows - y2\n",
    "                elif angle == 90:\n",
    "                    bbox['x1'] = rows - y2\n",
    "                    bbox['x2'] = rows - y1\n",
    "                    bbox['y1'] = x1\n",
    "                    bbox['y2'] = x2        \n",
    "                elif angle == 0:\n",
    "                    pass\n",
    "\n",
    "    img_data_aug['width'] = img.shape[1]\n",
    "    img_data_aug['height'] = img.shape[0]\n",
    "    return img_data_aug, img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kk14GTaNmqoo"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oNsi6HtyJPSb"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C66bqGuOq7w6"
   },
   "outputs": [],
   "source": [
    "base_path = './'\n",
    "\n",
    "test_path = 'validation_input.txt' # Test data (annotation file)\n",
    "\n",
    "test_base_path = 'validation' # Directory to save the test images\n",
    "\n",
    "config_output_filename = os.path.join(base_path, 'model_vgg_config.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hr7saGnTxC0S"
   },
   "outputs": [],
   "source": [
    "with open(config_output_filename, 'rb') as f_in:\n",
    "    C = pickle.load(f_in)\n",
    "\n",
    "# turn off any data augmentation at test time\n",
    "C.use_horizontal_flips = False\n",
    "C.use_vertical_flips = False\n",
    "C.rot_90 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SJTc51uyFrKc"
   },
   "source": [
    "# Measure mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U1J6MchWFjBX"
   },
   "outputs": [],
   "source": [
    "def format_img_size(img, C):\n",
    "    \"\"\" formats the image size based on config \"\"\"\n",
    "    img_min_side = float(C.im_size)\n",
    "    (height,width,_) = img.shape\n",
    "\n",
    "    if width <= height:\n",
    "        ratio = img_min_side/width\n",
    "        new_height = int(ratio * height)\n",
    "        new_width = int(img_min_side)\n",
    "    else:\n",
    "        ratio = img_min_side/height\n",
    "        new_width = int(ratio * width)\n",
    "        new_height = int(img_min_side)\n",
    "    img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n",
    "    return img, ratio\n",
    "\n",
    "def format_img_channels(img, C):\n",
    "    \"\"\" formats the image channels based on config \"\"\"\n",
    "    img = img[:, :, (2, 1, 0)]\n",
    "    img = img.astype(np.float32)\n",
    "    img[:, :, 0] -= C.img_channel_mean[0]\n",
    "    img[:, :, 1] -= C.img_channel_mean[1]\n",
    "    img[:, :, 2] -= C.img_channel_mean[2]\n",
    "    img /= C.img_scaling_factor\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "def format_img(img, C):\n",
    "    \"\"\" formats an image for model prediction based on config \"\"\"\n",
    "    img, ratio = format_img_size(img, C)\n",
    "    img = format_img_channels(img, C)\n",
    "    return img, ratio\n",
    "\n",
    "# Method to transform the coordinates of the bounding box to its original size\n",
    "def get_real_coordinates(ratio, x1, y1, x2, y2):\n",
    "\n",
    "    real_x1 = int(round(x1 // ratio))\n",
    "    real_y1 = int(round(y1 // ratio))\n",
    "    real_x2 = int(round(x2 // ratio))\n",
    "    real_y2 = int(round(y2 // ratio))\n",
    "\n",
    "    return (real_x1, real_y1, real_x2 ,real_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yLrF_sMQIRPR",
    "outputId": "e0b5b6a9-fb88-4f43-d6e3-5f028d287fc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\tensorflow2_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Loading weights from ./model/model_frcnn_vgg.hdf5\n"
     ]
    }
   ],
   "source": [
    "num_features = 512\n",
    "\n",
    "input_shape_img = (None, None, 3)\n",
    "input_shape_features = (None, None, num_features)\n",
    "\n",
    "img_input = Input(shape=input_shape_img)\n",
    "roi_input = Input(shape=(C.num_rois, 4))\n",
    "feature_map_input = Input(shape=input_shape_features)\n",
    "\n",
    "# define the base network (VGG here, can be Resnet50, Inception, etc)\n",
    "shared_layers = faster_rcnn.nn_base(img_input, trainable=True)\n",
    "\n",
    "# define the RPN, built on the base layers\n",
    "num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)\n",
    "rpn_layers = faster_rcnn.rpn_layer(shared_layers, num_anchors)\n",
    "\n",
    "classifier = faster_rcnn.classifier_layer(feature_map_input, roi_input, C.num_rois, nb_classes=len(C.class_mapping))\n",
    "\n",
    "model_rpn = Model(img_input, rpn_layers)\n",
    "model_classifier_only = Model([feature_map_input, roi_input], classifier)\n",
    "\n",
    "model_classifier = Model([feature_map_input, roi_input], classifier)\n",
    "\n",
    "print('Loading weights from {}'.format(C.model_path))\n",
    "model_rpn.load_weights('./model/Epoch_170.hdf5', by_name=True)\n",
    "model_classifier.load_weights('./model/Epoch_170.hdf5', by_name=True)\n",
    "\n",
    "model_rpn.compile(optimizer='sgd', loss='mse')\n",
    "model_classifier.compile(optimizer='sgd', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oMGXOYQbK_Rx",
    "outputId": "4a3f7d6a-7af5-4efe-cffd-4b6c9c1f3c85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'truck', 1: 'vehicle fallback', 2: 'car', 3: 'motorcycle', 4: 'rider', 5: 'person', 6: 'bus', 7: 'bicycle', 8: 'autorickshaw', 9: 'animal', 10: 'traffic sign', 11: 'train', 12: 'traffic light', 13: 'caravan', 14: 'trailer', 15: 'bg'}\n"
     ]
    }
   ],
   "source": [
    "# Switch key value for class mapping\n",
    "class_mapping = C.class_mapping\n",
    "class_mapping = {v: k for k, v in class_mapping.items()}\n",
    "print(class_mapping)\n",
    "class_to_color = {class_mapping[v]: np.random.randint(0, 255, 3) for v in class_mapping}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cX5OTM5Ppl0W",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4024\n",
      "2677\n",
      "6261\n",
      "2754\n",
      "5378\n",
      "4460\n",
      "3045\n",
      "5130\n",
      "2201\n",
      "3057\n",
      "1284\n",
      "1442\n"
     ]
    }
   ],
   "source": [
    "test_imgs = os.listdir(test_base_path)\n",
    "imgs_path = []\n",
    "for i in range(12):\n",
    "    idx = np.random.randint(len(test_imgs))\n",
    "    print(idx)\n",
    "    imgs_path.append(test_imgs[idx])\n",
    "\n",
    "all_imgs = []\n",
    "\n",
    "classes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HGt9OlDoU4vM"
   },
   "outputs": [],
   "source": [
    "def get_map(pred, gt, f):\n",
    "    T = {}\n",
    "    P = {}\n",
    "    fx, fy = f\n",
    "\n",
    "    for bbox in gt:\n",
    "        bbox['bbox_matched'] = False\n",
    "\n",
    "    pred_probs = np.array([s['prob'] for s in pred])\n",
    "    box_idx_sorted_by_prob = np.argsort(pred_probs)[::-1]\n",
    "\n",
    "    for box_idx in box_idx_sorted_by_prob:\n",
    "        pred_box = pred[box_idx]\n",
    "        pred_class = pred_box['class']\n",
    "        pred_x1 = pred_box['x1']\n",
    "        pred_x2 = pred_box['x2']\n",
    "        pred_y1 = pred_box['y1']\n",
    "        pred_y2 = pred_box['y2']\n",
    "        pred_prob = pred_box['prob']\n",
    "        if pred_class not in P:\n",
    "            P[pred_class] = []\n",
    "            T[pred_class] = []\n",
    "        P[pred_class].append(pred_prob)\n",
    "        found_match = False\n",
    "\n",
    "        for gt_box in gt:\n",
    "            gt_class = gt_box['class']\n",
    "            gt_x1 = gt_box['x1']/fx\n",
    "            gt_x2 = gt_box['x2']/fx\n",
    "            gt_y1 = gt_box['y1']/fy\n",
    "            gt_y2 = gt_box['y2']/fy\n",
    "            gt_seen = gt_box['bbox_matched']\n",
    "            if gt_class != pred_class:\n",
    "                continue\n",
    "            if gt_seen:\n",
    "                continue\n",
    "            iou_map = faster_rcnn.iou((pred_x1, pred_y1, pred_x2, pred_y2), (gt_x1, gt_y1, gt_x2, gt_y2))\n",
    "            if iou_map >= 0.5:\n",
    "                found_match = True\n",
    "                gt_box['bbox_matched'] = True\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        T[pred_class].append(int(found_match))\n",
    "\n",
    "    for gt_box in gt:\n",
    "        if not gt_box['bbox_matched']:# and not gt_box['difficult']:\n",
    "            if gt_box['class'] not in P:\n",
    "                P[gt_box['class']] = []\n",
    "                T[gt_box['class']] = []\n",
    "\n",
    "            T[gt_box['class']].append(1)\n",
    "            P[gt_box['class']].append(0)\n",
    "\n",
    "    #import pdb\n",
    "    #pdb.set_trace()\n",
    "    return T, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7m0J2BiEqLgA"
   },
   "outputs": [],
   "source": [
    "def format_img_map(img, C):\n",
    "    \"\"\"Format image for mAP. Resize original image to C.im_size (300 in here)\n",
    "\n",
    "    Args:\n",
    "        img: cv2 image\n",
    "        C: config\n",
    "\n",
    "    Returns:\n",
    "        img: Scaled and normalized image with expanding dimension\n",
    "        fx: ratio for width scaling\n",
    "        fy: ratio for height scaling\n",
    "    \"\"\"\n",
    "\n",
    "    img_min_side = float(C.im_size)\n",
    "    (height,width,_) = img.shape\n",
    "\n",
    "    if width <= height:\n",
    "        f = img_min_side/width\n",
    "        new_height = int(f * height)\n",
    "        new_width = int(img_min_side)\n",
    "    else:\n",
    "        f = img_min_side/height\n",
    "        new_width = int(f * width)\n",
    "        new_height = int(img_min_side)\n",
    "    fx = width/float(new_width)\n",
    "    fy = height/float(new_height)\n",
    "    img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n",
    "    # Change image channel from BGR to RGB\n",
    "    img = img[:, :, (2, 1, 0)]\n",
    "    img = img.astype(np.float32)\n",
    "    img[:, :, 0] -= C.img_channel_mean[0]\n",
    "    img[:, :, 1] -= C.img_channel_mean[1]\n",
    "    img[:, :, 2] -= C.img_channel_mean[2]\n",
    "    img /= C.img_scaling_factor\n",
    "    # Change img shape from (height, width, channel) to (channel, height, width)\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    # Expand one dimension at axis 0\n",
    "    # img shape becames (1, channel, height, width)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img, fx, fy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xNUNkonDqOwg",
    "outputId": "74331d3f-0826-4afc-b30b-f6e7088af662"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'truck', 1: 'vehicle fallback', 2: 'car', 3: 'motorcycle', 4: 'rider', 5: 'person', 6: 'bus', 7: 'bicycle', 8: 'autorickshaw', 9: 'animal', 10: 'traffic sign', 11: 'train', 12: 'traffic light', 13: 'caravan', 14: 'trailer', 15: 'bg'}\n"
     ]
    }
   ],
   "source": [
    "print(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "63-8osThqSjE",
    "outputId": "b9f9832c-df03-41f5-c14e-4f6737f5c34f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing annotation files\n",
      "idx=110440"
     ]
    }
   ],
   "source": [
    "# This might takes a while to parser the data\n",
    "test_imgs, _, _ = get_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35241
    },
    "colab_type": "code",
    "id": "RfNn-rWQsWPs",
    "outputId": "6b6c1072-5754-4c2c-f941-a5402d3bd8ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow2_gpu\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:657: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n"
     ]
    }
   ],
   "source": [
    "T = {}\n",
    "P = {}\n",
    "mAPs = []\n",
    "for idx, img_data in enumerate(test_imgs):\n",
    "    #print('{}/{}'.format(idx,len(test_imgs)))\n",
    "    st = time.time()\n",
    "    filepath = img_data['filepath']\n",
    "    #print(filepath)\n",
    "    img = cv2.imread(filepath)\n",
    "    #print(img)\n",
    "    X, fx, fy = format_img_map(img, C)\n",
    "\n",
    "    # Change X (img) shape from (1, channel, height, width) to (1, height, width, channel)\n",
    "    X = np.transpose(X, (0, 2, 3, 1))\n",
    "\n",
    "    # get the feature maps and output from the RPN\n",
    "    [Y1, Y2, F] = model_rpn.predict(X)\n",
    "\n",
    "\n",
    "    R = faster_rcnn.rpn_to_roi(Y1, Y2, C, overlap_thresh=0.7)\n",
    "\n",
    "    # convert from (x1,y1,x2,y2) to (x,y,w,h)\n",
    "    R[:, 2] -= R[:, 0]\n",
    "    R[:, 3] -= R[:, 1]\n",
    "\n",
    "    # apply the spatial pyramid pooling to the proposed regions\n",
    "    bboxes = {}\n",
    "    probs = {}\n",
    "\n",
    "    for jk in range(R.shape[0] // C.num_rois + 1):\n",
    "        ROIs = np.expand_dims(R[C.num_rois * jk:C.num_rois * (jk + 1), :], axis=0)\n",
    "        if ROIs.shape[1] == 0:\n",
    "            break\n",
    "\n",
    "        if jk == R.shape[0] // C.num_rois:\n",
    "            # pad R\n",
    "            curr_shape = ROIs.shape\n",
    "            target_shape = (curr_shape[0], C.num_rois, curr_shape[2])\n",
    "            ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)\n",
    "            ROIs_padded[:, :curr_shape[1], :] = ROIs\n",
    "            ROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]\n",
    "            ROIs = ROIs_padded\n",
    "\n",
    "        [P_cls, P_regr] = model_classifier_only.predict([F, ROIs])\n",
    "\n",
    "        # Calculate all classes' bboxes coordinates on resized image (300, 400)\n",
    "        # Drop 'bg' classes bboxes\n",
    "        for ii in range(P_cls.shape[1]):\n",
    "\n",
    "            # If class name is 'bg', continue\n",
    "            if np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1):\n",
    "                continue\n",
    "\n",
    "            # Get class name\n",
    "            cls_name = class_mapping[np.argmax(P_cls[0, ii, :])]\n",
    "\n",
    "            if cls_name not in bboxes:\n",
    "                bboxes[cls_name] = []\n",
    "                probs[cls_name] = []\n",
    "\n",
    "            (x, y, w, h) = ROIs[0, ii, :]\n",
    "\n",
    "            cls_num = np.argmax(P_cls[0, ii, :])\n",
    "            try:\n",
    "                (tx, ty, tw, th) = P_regr[0, ii, 4 * cls_num:4 * (cls_num + 1)]\n",
    "                tx /= C.classifier_regr_std[0]\n",
    "                ty /= C.classifier_regr_std[1]\n",
    "                tw /= C.classifier_regr_std[2]\n",
    "                th /= C.classifier_regr_std[3]\n",
    "                x, y, w, h = roi_helpers.apply_regr(x, y, w, h, tx, ty, tw, th)\n",
    "            except:\n",
    "                pass\n",
    "            bboxes[cls_name].append([16 * x, 16 * y, 16 * (x + w), 16 * (y + h)])\n",
    "            probs[cls_name].append(np.max(P_cls[0, ii, :]))\n",
    "\n",
    "    all_dets = []\n",
    "\n",
    "    for key in bboxes:\n",
    "        bbox = np.array(bboxes[key])\n",
    "\n",
    "        # Apply non-max-suppression on final bboxes to get the output bounding boxe\n",
    "        new_boxes, new_probs = faster_rcnn.non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=0.5)\n",
    "        for jk in range(new_boxes.shape[0]):\n",
    "            (x1, y1, x2, y2) = new_boxes[jk, :]\n",
    "            det = {'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2, 'class': key, 'prob': new_probs[jk]}\n",
    "            all_dets.append(det)\n",
    "\n",
    "\n",
    "    #print('Elapsed time = {}'.format(time.time() - st))\n",
    "    t, p = get_map(all_dets, img_data['bboxes'], (fx, fy))\n",
    "    for key in t.keys():\n",
    "        if key not in T:\n",
    "            T[key] = []\n",
    "            P[key] = []\n",
    "        T[key].extend(t[key])\n",
    "        P[key].extend(p[key])\n",
    "    all_aps = []\n",
    "    for key in T.keys():\n",
    "        ap = average_precision_score(T[key], P[key])\n",
    "        #print('{} AP: {}'.format(key, ap))\n",
    "        all_aps.append(ap)\n",
    "    #print('mAP = {}'.format(np.mean(np.array(all_aps))))\n",
    "    mAPs.append(np.mean(np.array(all_aps)))\n",
    "    #print(T)\n",
    "    #print(P)\n",
    "    \n",
    "#print()\n",
    "#print('mean average precision:', np.mean(np.array(mAPs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oVBClYzBZtZV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean average precision is 0.542\n"
     ]
    }
   ],
   "source": [
    "mAP = [mAP for mAP in mAPs if str(mAP)!='nan']\n",
    "mean_average_prec = round(np.mean(np.array(mAP)), 3)\n",
    "print('The mean average precision is %0.3f'%(mean_average_prec))\n",
    "\n",
    "# record_df.loc[len(record_df)-1, 'mAP'] = mean_average_prec\n",
    "# record_df.to_csv(C.record_path, index=0)\n",
    "# print('Save mAP to {}'.format(C.record_path))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "frcnn_test_vgg.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
